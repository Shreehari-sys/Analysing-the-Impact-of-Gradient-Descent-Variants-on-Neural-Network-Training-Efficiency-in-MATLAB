# Analysing-the-Impact-of-Gradient-Descent-Variants-on-Neural-Network-Training-Efficiency-in-MATLAB
Handwritten digit classification using neural networks

Stochastic Gradient Descent with Momentum (SGDM), Batch Gradient Descent, Mini-batch Gradient Descent, and the Adam optimizer represent different approaches to neural network training, offering various trade-offs between computational efficiency and convergence accuracy.

Results demonstrate varying performance across optimizers, with Adam achieving the highest accuracy, followed by Mini-batch Gradient Descent, while maintaining computational efficiency.

# Dataset Description
The dataset used in this study is the MNIST database containing 70,000 grayscale images of handwritten digits (28x28 pixels).

# Link to Dataset:

